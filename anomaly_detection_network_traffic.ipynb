{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ab1ba9",
   "metadata": {},
   "source": [
    "# Anomaly Detection in Network Traffic\n",
    "\n",
    "This notebook demonstrates the process of detecting anomalies in network traffic using the KDD Cup 1999 dataset. We'll go through the following steps:\n",
    "\n",
    "1. Data Collection and Preprocessing\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Clustering and Anomaly Detection\n",
    "4. Visualization of Results\n",
    "\n",
    "Let's begin by importing the necessary libraries and loading our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f631381",
   "metadata": {},
   "source": [
    "# 1. Data Collection and Preprocessing\n",
    "\n",
    "In this section, we'll load the KDD Cup 1999 dataset, preprocess it, and prepare it for our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d7452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the KDD Cup 1999 dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data_10_percent.gz\"\n",
    "columns = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \n",
    "           \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \n",
    "           \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \n",
    "           \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \n",
    "           \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \n",
    "           \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \n",
    "           \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \n",
    "           \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \n",
    "           \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"]\n",
    "\n",
    "df = pd.read_csv(url, names=columns)\n",
    "\n",
    "# Preprocess the data\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 'normal.' else 1)  # Binary classification (normal vs anomaly)\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "df['protocol_type'] = le.fit_transform(df['protocol_type'])\n",
    "df['service'] = le.fit_transform(df['service'])\n",
    "df['flag'] = le.fit_transform(df['flag'])\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df.drop(['label'], axis=1))\n",
    "\n",
    "# Split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data preprocessing completed. Shape of training set:\", X_train.shape)\n",
    "print(\"Shape of test set:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ac2a9",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now that we have preprocessed our data, let's visualize it to gain some insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84472909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the label distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(df['label'])\n",
    "plt.title('Label Distribution: Normal vs Anomalies')\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dde97a",
   "metadata": {},
   "source": [
    "# 3. Clustering and Anomaly Detection\n",
    "\n",
    "In this section, we'll apply three different methods for anomaly detection:\n",
    "1. DBSCAN\n",
    "2. Isolation Forest\n",
    "3. Autoencoder\n",
    "\n",
    "We'll evaluate each method using classification reports and confusion matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfbf2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "y_pred_dbscan = dbscan.fit_predict(X_test)\n",
    "\n",
    "print(\"DBSCAN - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dbscan))\n",
    "print(\"DBSCAN - Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_dbscan))\n",
    "\n",
    "# Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "y_pred_iso_forest = iso_forest.fit_predict(X_test)\n",
    "\n",
    "print(\"\n",
    "Isolation Forest - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_iso_forest))\n",
    "print(\"Isolation Forest - Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_iso_forest))\n",
    "\n",
    "# Autoencoder\n",
    "input_dim = X_train.shape[1]\n",
    "autoencoder = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(input_dim,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(input_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train, X_train, epochs=10, batch_size=64, validation_data=(X_test, X_test), verbose=0)\n",
    "\n",
    "reconstruction = autoencoder.predict(X_test)\n",
    "reconstruction_error = tf.reduce_mean(tf.square(X_test - reconstruction), axis=1)\n",
    "threshold = tf.reduce_mean(reconstruction_error) + 2 * tf.math.reduce_std(reconstruction_error)\n",
    "y_pred_autoencoder = tf.where(reconstruction_error > threshold, 1, 0)\n",
    "\n",
    "print(\"\n",
    "Autoencoder - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_autoencoder))\n",
    "print(\"Autoencoder - Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_autoencoder))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767089e3",
   "metadata": {},
   "source": [
    "# 4. Visualization of Results\n",
    "\n",
    "Finally, let's visualize our results using PCA to reduce the dimensionality of our data to 2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7dd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=y_pred_dbscan, cmap='coolwarm', alpha=0.7)\n",
    "plt.title('DBSCAN - PCA Visualization of Clusters and Anomalies')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41127407",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the process of anomaly detection in network traffic using the KDD Cup 1999 dataset. We've applied three different methods: DBSCAN, Isolation Forest, and Autoencoder.\n",
    "\n",
    "Each method has its strengths and weaknesses:\n",
    "- DBSCAN is good at identifying clusters of normal behavior and flagging outliers as anomalies.\n",
    "- Isolation Forest is efficient for high-dimensional data and can handle large datasets well.\n",
    "- Autoencoder can capture complex patterns in the data and is flexible in terms of architecture design.\n",
    "\n",
    "The choice of method depends on the specific requirements of your use case, such as interpretability, scalability, and the nature of your data.\n",
    "\n",
    "To improve this analysis, you could:\n",
    "1. Try different hyperparameters for each method\n",
    "2. Combine multiple methods for ensemble learning\n",
    "3. Use more advanced deep learning techniques like LSTM autoencoders for sequence data\n",
    "4. Incorporate domain knowledge to engineer more relevant features\n",
    "\n",
    "Remember to always validate your results and consider the practical implications of your anomaly detection system in a real-world network security context.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
